{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Building a GPT"
   ],
   "metadata": {
    "id": "wJpXpmjEYC_T"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from src.data import Data, Vocabulary\n",
    "from src.gpt import GPTConfig, GPTModel\n",
    "from src.utils import count_parameters, estimate_loss, get_tinyshakespeare_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.152705Z",
     "start_time": "2024-03-27T19:20:47.864473Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text = get_tinyshakespeare_dataset()"
   ],
   "metadata": {
    "id": "O6medjfRsLD9",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.378606Z",
     "start_time": "2024-03-27T19:20:51.156203Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xWI_VyAsN8F",
    "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.385295Z",
     "start_time": "2024-03-27T19:20:51.380773Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c5V0FvqseE0",
    "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.394771Z",
     "start_time": "2024-03-27T19:20:51.388285Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "vocab = Vocabulary(text=text)\n",
    "print(''.join(vocab.stoi.keys()))\n",
    "print(vocab.size)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e-Rbyr8sfM8",
    "outputId": "f34e94a9-5b44-4cf3-885b-986731929109",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.429250Z",
     "start_time": "2024-03-27T19:20:51.407508Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(vocab.encode(\"hii there\"))\n",
    "print(vocab.decode(vocab.encode(\"hii there\")))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yw1LKNCgwjj1",
    "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.444994Z",
     "start_time": "2024-03-27T19:20:51.436944Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = Data(vocab.encode(text))\n",
    "print(data.train_data.shape, data.train_data.dtype)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJb0OXPwzvqg",
    "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.645766Z",
     "start_time": "2024-03-27T19:20:51.447551Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([892315]) torch.int64\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 32  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "max_iters = 10001\n",
    "eval_interval = 1000\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "emb_size = 32\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "dropout = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:51.652204Z",
     "start_time": "2024-03-27T19:20:51.647647Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=vocab.size,\n",
    "    block_size=block_size,\n",
    "    emb_size=emb_size,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ")\n",
    "model = GPTModel(config=config)\n",
    "print(f'models has {count_parameters(model)} parameters')\n",
    "\n",
    "print(vocab.decode(model.generate(start_tokens=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:54.313389Z",
     "start_time": "2024-03-27T19:20:51.655187Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models has 42369 parameters\n",
      "\n",
      "Eg?cAKZh&RgRtuYjm3ySZQi:FRTXRckiVzvUKzNAkx!EVNcqf.vBv?ZRpQW:YxQJiYk.v m dd UR:;UbRGF,&'D!,YT$YeFWSRq\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "id": "eTyJ8qAaDdiF",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:20:54.322247Z",
     "start_time": "2024-03-27T19:20:54.315437Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "timer = time()\n",
    "for steps in range(max_iters):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = data.get_batch('train', block_size=block_size, batch_size=batch_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % eval_interval == 0:\n",
    "        losses = estimate_loss(\n",
    "            eval_iters=eval_iters, model=model, data=data, block_size=block_size, batch_size=batch_size\n",
    "        )\n",
    "        print(\n",
    "            f\"step: {steps + 1}; training time: {round(time() - timer)} sec; train loss: {losses['train']:.4f}; val loss: {losses['val']:.4f}\"\n",
    "        )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "42ded55c-2983-4d91-c528-675b2edfa849",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:24:40.182167Z",
     "start_time": "2024-03-27T19:20:54.324938Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1; training time: 2 sec; train loss: 4.3260; val loss: 4.3328\n",
      "step: 1001; training time: 24 sec; train loss: 2.3468; val loss: 2.3450\n",
      "step: 2001; training time: 47 sec; train loss: 2.2142; val loss: 2.2592\n",
      "step: 3001; training time: 70 sec; train loss: 2.1532; val loss: 2.2005\n",
      "step: 4001; training time: 92 sec; train loss: 2.1034; val loss: 2.1526\n",
      "step: 5001; training time: 114 sec; train loss: 2.0845; val loss: 2.1482\n",
      "step: 6001; training time: 137 sec; train loss: 2.0736; val loss: 2.1401\n",
      "step: 7001; training time: 159 sec; train loss: 2.0439; val loss: 2.1016\n",
      "step: 8001; training time: 181 sec; train loss: 2.0162; val loss: 2.0986\n",
      "step: 9001; training time: 204 sec; train loss: 2.0154; val loss: 2.0878\n",
      "step: 10001; training time: 226 sec; train loss: 2.0129; val loss: 2.0915\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(vocab.decode(model.generate(start_tokens=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b",
    "ExecuteTime": {
     "end_time": "2024-03-27T19:24:42.416517Z",
     "start_time": "2024-03-27T19:24:40.184894Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And hoime, will; feas,\n",
      "What bewat sest be neicle morge his\n",
      "lowelf apportyal the;\n",
      "As the deice.\n",
      "\n",
      "QUCASdriedis uplive,\n",
      "Whit is mor youghen exetiis\n",
      "Till show! I arpea! conows?\n",
      "So for ond, on is Twereight honge, duction. Tare can on himers.\n",
      "What thy engy may as veand. I grearbe at,--fher, me net, us! Veremenew ong bir I kname,\n",
      "Cord.\n",
      "Shill himghty\n",
      "he and stis that likty hing?\n",
      "\n",
      "Mosed thrathal you digh peat const\n",
      "More weareny'd now lose plence array,\n",
      "Mitsed me kBRingt you there you.\n",
      "\n",
      "Sir thewer, earry'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T19:24:42.424010Z",
     "start_time": "2024-03-27T19:24:42.418460Z"
    }
   },
   "execution_count": 12
  }
 ]
}
